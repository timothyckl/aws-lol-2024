{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f549b54-40f7-466b-8531-960ac8a41c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from time import time, sleep\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import Union, Sequence, Optional\n",
    "\n",
    "from openai import OpenAI, OpenAIError\n",
    "from mistralai.client import MistralClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0715f21c-1bbe-4d92-bc04-58d0354a235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "I am preparing a set of exam questions about machine learning theory. \n",
      "Generate 50 questions on machine learning theory for me in this jsonl format: \n",
      "{\"instruction\": <Insert exam question>,\"context\":\"\",\"response\":\"<insert response here>} \n",
      "Each json should be begin at a brand new line.\n",
      "\n",
      "Questions:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SelfInstruct:\n",
    "  def __init__(\n",
    "      self,  \n",
    "      api_key=None,\n",
    "      model_provider=\"openai\",\n",
    "      model_name=\"text-davinci-003\",\n",
    "      prompt_template_path=\"./prompt.txt\",\n",
    "      seed_tasks_path=\"./seed_tasks.jsonl\", \n",
    "      num_instructions_to_generate=100,\n",
    "      num_prompt_instructions=3,\n",
    "      request_batch_size=5,\n",
    "      num_cpus=16\n",
    "    ):\n",
    "\n",
    "    if model_provider == \"openai\":\n",
    "      self.client = OpenAI(api_key=api_key)\n",
    "    elif model_provider == \"mistral\":\n",
    "      self.client = MistralClient(api_key=api_key, model=model_name)\n",
    "    else:\n",
    "      raise NotImplementedError(f\"Model provider {model_provider} not implemented.\")\n",
    "\n",
    "    self.prompt_template_path = prompt_template_path\n",
    "    self.prompt_template = None\n",
    "    self.seed_tasks_path = seed_tasks_path\n",
    "    self.num_instructions_to_generate = num_instructions_to_generate\n",
    "    self.model_provider = model_provider\n",
    "    self.model_name = model_name\n",
    "    self.num_prompt_instructions = num_prompt_instructions\n",
    "    self.request_batch_size = request_batch_size\n",
    "    self.num_cpus = num_cpus\n",
    "\n",
    "  def configure_prompt(self, topic, difficulty):\n",
    "    \"\"\"Configure the prompt template.\"\"\"\n",
    "    prompt = open(self.prompt_template_path).read() + \"\\n\"\n",
    "    prompt = prompt.format(\n",
    "        num_questions=self.num_instructions_to_generate,\n",
    "        topic=topic,\n",
    "        difficulty=difficulty,\n",
    "    )\n",
    "\n",
    "    self.prompt_template = prompt\n",
    "\n",
    "  def encode_prompt(self, prompt_instructions):\n",
    "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
    "    prompt = self.prompt_template + \"\\n\"\n",
    "\n",
    "    for idx, task_dict in enumerate(prompt_instructions):\n",
    "        (instruction, output) = task_dict[\"instruction\"], task_dict[\"output\"]\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        prompt += f\"###\\n\"\n",
    "        prompt += f\"{idx + 1}. Instruction: {instruction}\\n\"\n",
    "        prompt += f\"{idx + 1}. Output: {output}\\n\"\n",
    "    prompt += f\"###\\n\"\n",
    "    prompt += f\"{idx + 2}. Instruction:\"\n",
    "    return prompt  \n",
    "\n",
    "  def openai_completion(\n",
    "      self, \n",
    "      prompts: Union[str, Sequence[str], Sequence[dict[str, str]], dict[str, str]],\n",
    "      decoding_args={},\n",
    "      batch_size=1,\n",
    "      max_instances=sys.maxsize,\n",
    "      sleep_time=2,\n",
    "      ):\n",
    "    is_single_prompt = isinstance(prompts, (str, dict))\n",
    "    \n",
    "    if is_single_prompt:\n",
    "        prompts = [prompts]\n",
    "\n",
    "    prompts = prompts[:max_instances]\n",
    "    num_prompts = len(prompts)\n",
    "    prompt_batches = [\n",
    "        prompts[batch_id * batch_size : (batch_id + 1) * batch_size]\n",
    "        for batch_id in range(int(math.ceil(num_prompts / batch_size)))\n",
    "    ]\n",
    "\n",
    "    completions = []\n",
    "    for batch_id, prompt_batch in tqdm(\n",
    "        enumerate(prompt_batches),\n",
    "        desc=\"prompt_batches\",\n",
    "        total=len(prompt_batches),\n",
    "    ):\n",
    "\n",
    "      batch_decoding_args = decoding_args\n",
    "\n",
    "      while True:\n",
    "        try:\n",
    "          # batched completion requests\n",
    "          completion_batch = self.client.completions.create(prompt=prompt_batch, model=self.model_name, **batch_decoding_args)\n",
    "          choices = completion_batch.choices\n",
    "\n",
    "          for choice in choices:\n",
    "            choice[\"total_tokens\"] = completion_batch.usage.total_tokens\n",
    "\n",
    "          completions.extend(choices)\n",
    "          break\n",
    "\n",
    "        except OpenAIError as e:\n",
    "          logging.warning(f\"OpenAIError: {e}.\")\n",
    "          if \"Please reduce your prompt\" in str(e):\n",
    "            batch_decoding_args.max_tokens = int(batch_decoding_args.max_tokens * 0.8)\n",
    "            logging.warning(f\"Reducing target length to {batch_decoding_args.max_tokens}, Retrying...\")\n",
    "          else:\n",
    "            logging.warning(\"Hit request rate limit; retrying...\")\n",
    "            sleep(sleep_time)  # Annoying rate limit on requests.\n",
    "\n",
    "    if decoding_args['n'] > 1:\n",
    "        # make completions a nested list, where each entry is a consecutive decoding_args.n of original entries.\n",
    "        completions = [completions[i : i + decoding_args['n']] for i in range(0, len(completions), decoding_args['n'])]\n",
    "    if is_single_prompt:\n",
    "        # Return non-tuple if only 1 input and 1 generation.\n",
    "        (completions,) = completions\n",
    "\n",
    "    return completions\n",
    "\n",
    "  def mistral_completion(self,):\n",
    "    # mistral api does not support batched requests\n",
    "    pass\n",
    "\n",
    "  def find_word_in_string(w, s):\n",
    "      return re.compile(r\"\\b({0})\\b\".format(w), flags=re.IGNORECASE).search(s)\n",
    "\n",
    "  def post_process_gpt3_response(self, num_prompt_instructions, response):\n",
    "    if response is None:\n",
    "        return []\n",
    "\n",
    "    raw_instructions = f\"{num_prompt_instructions+1}. Instruction:\" + response[\"text\"]\n",
    "    raw_instructions = re.split(\"###\", raw_instructions)\n",
    "    instructions = []\n",
    "\n",
    "    for idx, inst in enumerate(raw_instructions):\n",
    "        # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "        if idx == len(raw_instructions) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "            continue\n",
    "\n",
    "        idx += num_prompt_instructions + 1\n",
    "        splitted_data = re.split(f\"{idx}\\.\\s+(Instruction|Input|Output):\", inst)\n",
    "\n",
    "        if len(splitted_data) != 7:\n",
    "            continue\n",
    "        else:\n",
    "            inst = splitted_data[2].strip()\n",
    "            input = splitted_data[4].strip()\n",
    "            input = \"\" if input.lower() == \"<noinput>\" else input\n",
    "            output = splitted_data[6].strip()\n",
    "\n",
    "        # filter out too short or too long instructions\n",
    "        if len(inst.split()) <= 3 or len(inst.split()) > 150:\n",
    "            continue\n",
    "\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        blacklist = [\n",
    "            \"image\",\n",
    "            \"images\",\n",
    "            \"graph\",\n",
    "            \"graphs\",\n",
    "            \"picture\",\n",
    "            \"pictures\",\n",
    "            \"file\",\n",
    "            \"files\",\n",
    "            \"map\",\n",
    "            \"maps\",\n",
    "            \"draw\",\n",
    "            \"plot\",\n",
    "            \"go to\",\n",
    "            \"video\",\n",
    "            \"audio\",\n",
    "            \"music\",\n",
    "            \"flowchart\",\n",
    "            \"diagram\",\n",
    "        ]\n",
    "\n",
    "        blacklist += []\n",
    "\n",
    "        if any(self.find_word_in_string(word, inst) for word in blacklist):\n",
    "            continue\n",
    "        # We found that the model tends to add \"write a program\" to some existing instructions, which lead to a lot of such instructions.\n",
    "        # And it's a bit comfusing whether the model need to write a program or directly output the result.\n",
    "        # Here we filter them out.\n",
    "        # Note this is not a comprehensive filtering for all programming instructions.\n",
    "        if inst.startswith(\"Write a program\"):\n",
    "            continue\n",
    "        # filter those starting with punctuation\n",
    "        if inst[0] in punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not inst[0].isascii():\n",
    "            continue\n",
    "        instructions.append({\"instruction\": inst, \"input\": input, \"output\": output})\n",
    "\n",
    "    return instructions\n",
    "\n",
    "  def generate(self, output_dir='./'):\n",
    "    seed_tasks = [json.loads(l) for l in open(self.seed_tasks_path, \"r\")]\n",
    "    seed_instruction_data = [\n",
    "        {\"instruction\": t[\"instruction\"], \"output\": t[\"output\"]}\n",
    "        for t in seed_tasks\n",
    "    ]\n",
    "\n",
    "    print(f\"Loaded {len(seed_instruction_data)} human-written seed instructions\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    request_idx = 0\n",
    "\n",
    "    # load the LM-generated instructions (first run shouldn't trigger this)\n",
    "    machine_instruction_data = []\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir, \"regen.json\")):\n",
    "        machine_instruction_data = self.jload(os.path.join(output_dir, \"regen.json\"))\n",
    "        print(f\"Loaded {len(machine_instruction_data)} machine-generated instructions\")\n",
    "\n",
    "    # similarities\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "    # now let's generate new instructions!\n",
    "    progress_bar = tqdm(total=self.num_instructions_to_generate)\n",
    "\n",
    "    if machine_instruction_data:\n",
    "        progress_bar.update(len(machine_instruction_data))\n",
    "\n",
    "    # first we tokenize all the seed instructions and generated machine instructions\n",
    "    all_instructions = [d[\"instruction\"] for d in seed_instruction_data] + [\n",
    "        d[\"instruction\"] for d in machine_instruction_data\n",
    "    ]\n",
    "    all_instruction_tokens = [scorer._tokenizer.tokenize(inst) for inst in all_instructions]\n",
    "\n",
    "    while len(machine_instruction_data) < self.num_instructions_to_generate:\n",
    "        request_idx += 1\n",
    "        batch_inputs = []\n",
    "\n",
    "        for _ in range(self.request_batch_size):\n",
    "          # only sampling from the seed tasks\n",
    "          prompt_instructions = sample(seed_instruction_data, self.num_prompt_instructions)\n",
    "          prompt = self.encode_prompt(prompt_instructions)\n",
    "\n",
    "          batch_inputs.append(prompt)\n",
    "\n",
    "        request_start = time()\n",
    "\n",
    "        if self.model_provider == \"openai\":\n",
    "          decoding_args = {\n",
    "              \"temperature\": 0.7,\n",
    "              \"n\": 1,\n",
    "              \"max_tokens\": 3072,\n",
    "              \"top_p\": 1.0,\n",
    "              # \"stop\": [\"\\n20\", \"20.\", \"20.\"],\n",
    "          }\n",
    "\n",
    "          results = self.openai_completion(\n",
    "              prompts=batch_inputs,\n",
    "              decoding_args=decoding_args,\n",
    "          )\n",
    "\n",
    "          request_duration = time() - request_start\n",
    "          process_start = time()\n",
    "          \n",
    "          instruction_data = []\n",
    "\n",
    "          for result in results:\n",
    "            new_instructions = self.post_process_gpt3_response(self.num_prompt_instructions, result)\n",
    "            instruction_data += new_instructions\n",
    "\n",
    "          total = len(instruction_data)\n",
    "          keep = 0\n",
    "\n",
    "          for instruction_data_entry in instruction_data:\n",
    "            # computing similarity with the pre-tokenzied instructions\n",
    "            new_instruction_tokens = scorer._tokenizer.tokenize(instruction_data_entry[\"instruction\"])\n",
    "            with Pool(self.num_cpus) as p:\n",
    "                rouge_scores = p.map(\n",
    "                    partial(rouge_scorer._score_lcs, new_instruction_tokens),\n",
    "                    all_instruction_tokens,\n",
    "                )\n",
    "            \n",
    "            rouge_scores = [score.fmeasure for score in rouge_scores]\n",
    "            most_similar_instructions = {\n",
    "                all_instructions[i]: rouge_scores[i] for i in np.argsort(rouge_scores)[-10:][::-1]\n",
    "            }\n",
    "\n",
    "            if max(rouge_scores) > 0.7:\n",
    "                continue\n",
    "            else:\n",
    "                keep += 1\n",
    "\n",
    "            instruction_data_entry[\"most_similar_instructions\"] = most_similar_instructions\n",
    "            instruction_data_entry[\"avg_similarity_score\"] = float(np.mean(rouge_scores))\n",
    "            machine_instruction_data.append(instruction_data_entry)\n",
    "            all_instructions.append(instruction_data_entry[\"instruction\"])\n",
    "            all_instruction_tokens.append(new_instruction_tokens)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "          process_duration = time() - process_start\n",
    "          print(f\"Request {request_idx} took {request_duration:.2f}s, processing took {process_duration:.2f}s\")\n",
    "          print(f\"Generated {total} instructions, kept {keep} instructions\")\n",
    "          self.jdump(machine_instruction_data, os.path.join(output_dir, \"regen.json\"))\n",
    "\n",
    "        elif self.model_provider == \"mistral\":\n",
    "          raise NotImplementedError(\"Mistral model provider not implemented yet\")\n",
    "        else:\n",
    "          raise NotImplementedError(f\"Model provider {self.model_provider} not implemented\")\n",
    "\n",
    "  def _make_r_io_base(self, f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "  def jdump(self, obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = self._make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "  def jload(self, f, mode=\"r\"):\n",
    "      \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "      f = self._make_r_io_base(f, mode)\n",
    "      jdict = json.load(f)\n",
    "      f.close()\n",
    "      return jdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a205eb-50f4-45cd-9eff-cf2c16b9dbbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today? Is there something you would like to talk about or ask me? I'm here to help answer any questions you might have to the best of my ability. I can provide information on a wide range of topics, including science, history, technology, culture, and more. I can also help with tasks such as generating ideas, solving problems, and providing explanations. Let me know how I can assist you.\n"
     ]
    }
   ],
   "source": [
    "self_instruct = SelfInstruct(\n",
    "    api_key=\"asdasdasd\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"text-davinci-003\",\n",
    "    prompt_template_path=\"./prompt.txt\",\n",
    "    seed_tasks_path=\"/content/seed_tasks.jsonl\",\n",
    "    num_instructions_to_generate=100\n",
    ")\n",
    "\n",
    "self_instruct.configure_prompt(\n",
    "    topic='Machine Learning', \n",
    "    difficulty='Beginner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449eee17-c36c-4546-9a87-f9345fea0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_instruct.generate(output_dir=\"./\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
