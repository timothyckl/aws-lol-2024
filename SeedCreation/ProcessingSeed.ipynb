{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the fundamental goal of machine learning?</td>\n",
       "      <td>The fundamental goal of machine learning is to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain the concept of overfitting in machine ...</td>\n",
       "      <td>Overfitting occurs when a machine learning mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you distinguish between bias and variance ...</td>\n",
       "      <td>Bias refers to the error introduced by approxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of a loss function in machine...</td>\n",
       "      <td>A loss function measures the difference betwee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the concept of gradient descent.</td>\n",
       "      <td>Gradient descent is an optimization algorithm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is the purpose of the loss function in ma...</td>\n",
       "      <td>The loss function measures the difference betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is the role of a hyperparameter in machin...</td>\n",
       "      <td>Hyperparameters are parameters that are set be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Explain the concept of a decision tree in mach...</td>\n",
       "      <td>A decision tree is a tree-like model that make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How does a confusion matrix contribute to eval...</td>\n",
       "      <td>A confusion matrix is a table that summarizes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is the difference between regression and ...</td>\n",
       "      <td>Regression involves predicting a continuous ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0   What is the fundamental goal of machine learning?   \n",
       "1   Explain the concept of overfitting in machine ...   \n",
       "2   Can you distinguish between bias and variance ...   \n",
       "3   What is the role of a loss function in machine...   \n",
       "4            Explain the concept of gradient descent.   \n",
       "..                                                ...   \n",
       "95  What is the purpose of the loss function in ma...   \n",
       "96  What is the role of a hyperparameter in machin...   \n",
       "97  Explain the concept of a decision tree in mach...   \n",
       "98  How does a confusion matrix contribute to eval...   \n",
       "99  What is the difference between regression and ...   \n",
       "\n",
       "                                               Answer  \n",
       "0   The fundamental goal of machine learning is to...  \n",
       "1   Overfitting occurs when a machine learning mod...  \n",
       "2   Bias refers to the error introduced by approxi...  \n",
       "3   A loss function measures the difference betwee...  \n",
       "4   Gradient descent is an optimization algorithm ...  \n",
       "..                                                ...  \n",
       "95  The loss function measures the difference betw...  \n",
       "96  Hyperparameters are parameters that are set be...  \n",
       "97  A decision tree is a tree-like model that make...  \n",
       "98  A confusion matrix is a table that summarizes ...  \n",
       "99  Regression involves predicting a continuous ou...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('ML-101-QandA.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename answer column to 'topic' and question column to 'instruction'\n",
    "df = df.rename(columns={'Answer': 'output', 'Question': 'instruction'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ML rate the difficulty of each Instruction from 1 to 4 and add a new column called 'difficulty'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in c:\\users\\admin\\.conda\\envs\\gpu_envv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\.conda\\envs\\gpu_envv\\lib\\site-packages (from jsonlines) (23.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goal of Machine Learning',\n",
       " 'Overfitting',\n",
       " 'Bias and variance',\n",
       " 'Loss Function',\n",
       " 'Gradient Descent',\n",
       " 'Regularization',\n",
       " 'Classification vs. Regression',\n",
       " 'Decision Trees',\n",
       " 'Cross-validation',\n",
       " 'Ensemble Learning',\n",
       " 'Bias-Variance Tradeoff',\n",
       " 'Feature Engineering',\n",
       " 'Transfer Learning',\n",
       " 'SGD vs. Batch GD',\n",
       " 'Neural Networks',\n",
       " 'Hyperparameters',\n",
       " 'Kernel Functions in SVM',\n",
       " 'Activation Functions',\n",
       " 'K-Nearest Neighbors (KNN)',\n",
       " 'Curse of Dimensionality',\n",
       " 'Bagging',\n",
       " 'Random Forest',\n",
       " 'Activation Functions',\n",
       " 'Handling Missing Data',\n",
       " 'Bias in ML Algorithms',\n",
       " 'Confusion Matrix',\n",
       " 'Batch Normalization',\n",
       " 'Precision and Recall',\n",
       " 'Learning Rate in Gradient Descent',\n",
       " 'ROC Curve',\n",
       " 'Unsupervised vs. Semi-supervised Learning',\n",
       " 'Cross-entropy Loss',\n",
       " 'Hyperparameter Tuning and Grid Search',\n",
       " 'Sparsity in ML',\n",
       " 'Regularization in Linear Regression',\n",
       " 'Precision, Recall, F1 Score',\n",
       " 'Curse of Dimensionality',\n",
       " 'Transfer Learning',\n",
       " 'Decision Trees Advantages and Disadvantages',\n",
       " 'Kernel Trick in SVM',\n",
       " 'k-fold Cross-Validation',\n",
       " 'Bias-Variance Tradeoff',\n",
       " 'Batch GD vs. SGD',\n",
       " 'PCA for Dimensionality Reduction',\n",
       " 'Activation Functions in DNNs',\n",
       " 'RNNs vs. Feedforward NNs',\n",
       " 'Dropout in NNs',\n",
       " 'Batch Normalization',\n",
       " 'Rocchio Algorithm in Text Classification',\n",
       " 'SVM Advantages and Disadvantages',\n",
       " 'Learning Rate in Gradient Descent',\n",
       " 'Underfitting and Overfitting',\n",
       " 'Feature Scaling',\n",
       " 'Imbalanced Classes',\n",
       " 'Confusion Matrix and Accuracy',\n",
       " 'K-Means Clustering',\n",
       " 'Feature Importance',\n",
       " 'Bagging vs. Boosting',\n",
       " 'One-Hot Encoding',\n",
       " 'ROC Curve and AUC',\n",
       " 'Bias in ML Algorithms',\n",
       " 'Feature Selection',\n",
       " 'Adam Optimization Algorithm',\n",
       " 'Dropout Regularization',\n",
       " 'Cross-Entropy Loss in Binary vs. Multi-class Classification',\n",
       " 'Advantages and Disadvantages of Deep Learning',\n",
       " 'Handling Time-Series Data',\n",
       " 'Precision vs. Recall',\n",
       " 'Stratified Sampling',\n",
       " 'Activation Functions in Hidden Layers',\n",
       " 'L1 and L2 Regularization',\n",
       " 'Feature Extraction',\n",
       " 'Dropout in CNNs',\n",
       " 'Word Embeddings in NLP',\n",
       " 'Transfer Learning in Image Classification',\n",
       " 'Handling Missing Data Challenges and Solutions',\n",
       " 'Self-supervised Learning vs. Supervised Learning',\n",
       " 'Activation Functions in Output Layer for Binary Classification',\n",
       " 'Precision-Recall Tradeoff',\n",
       " 'Naive Bayes in Text Classification',\n",
       " 'Bag-of-Words',\n",
       " 'Model Interpretability',\n",
       " 'Class Imbalance and Strategies',\n",
       " 'Kernel in SVMs',\n",
       " 'Imputation for Missing Data',\n",
       " 'Autoencoders in Unsupervised Learning',\n",
       " 'Word Frequency in NLP',\n",
       " 'Cross-validation',\n",
       " 'Early Stopping in Neural Networks',\n",
       " 'Activation Function in Neural Networks',\n",
       " 'Introduction to Machine Learning',\n",
       " 'Types of Machine Learning',\n",
       " 'Feature and Label in ML',\n",
       " 'Training and Testing in ML',\n",
       " 'Overfitting in ML',\n",
       " 'Purpose of Loss Function in ML',\n",
       " 'Hyperparameters in ML',\n",
       " 'Decision Trees in ML',\n",
       " 'Confusion Matrix in ML',\n",
       " 'Regression vs. Classification in ML']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the reply.txt\n",
    "with open('reply.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# for each line Split Topic: and take the second part\n",
    "\n",
    "topics_difficulty = []\n",
    "for line in lines:\n",
    "    if 'Topic:' in line:\n",
    "        topic = line.split('Topic:')[1].strip()\n",
    "        topics_difficulty.append(topic)\n",
    "    else:\n",
    "        topics_difficulty.append('')\n",
    "\n",
    "# for each topic_difficulty split by \";\" the first part append to topics and the second part append to difficulty\n",
    "topics = []\n",
    "difficulty = []\n",
    "for topic_difficulty in topics_difficulty:\n",
    "    if topic_difficulty:\n",
    "        topic, diff = topic_difficulty.split(';')\n",
    "        topics.append(topic.strip())\n",
    "        difficulty.append(diff.strip())\n",
    "    else:\n",
    "        topics.append('')\n",
    "        difficulty.append('')\n",
    "\n",
    "# remove all empty strings from topics list\n",
    "topics = list(filter(None, topics))\n",
    "topics\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip the Difficulty: and take the second part and remove all empty string in the list\n",
    "difficulty = [diff.split('Difficulty:')[1].strip() for diff in difficulty if diff]\n",
    "# convert all to integer\n",
    "difficulty = [int(diff) for diff in difficulty]\n",
    "difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>topic</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the fundamental goal of machine learning?</td>\n",
       "      <td>The fundamental goal of machine learning is to...</td>\n",
       "      <td>Goal of Machine Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the difference between classification ...</td>\n",
       "      <td>Classification involves predicting categorical...</td>\n",
       "      <td>Classification vs. Regression</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is feature engineering, and why is it imp...</td>\n",
       "      <td>Feature engineering involves creating new feat...</td>\n",
       "      <td>Feature Engineering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is a hyperparameter in machine learning?</td>\n",
       "      <td>A hyperparameter is a configuration setting ex...</td>\n",
       "      <td>Hyperparameters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>What is machine learning?</td>\n",
       "      <td>Machine learning is a subset of artificial int...</td>\n",
       "      <td>Introduction to Machine Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Can you explain the difference between a featu...</td>\n",
       "      <td>In machine learning, a feature is an input var...</td>\n",
       "      <td>Feature and Label in ML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>What is the training and testing process in ma...</td>\n",
       "      <td>The training process involves using a labeled ...</td>\n",
       "      <td>Training and Testing in ML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is the difference between regression and ...</td>\n",
       "      <td>Regression involves predicting a continuous ou...</td>\n",
       "      <td>Regression vs. Classification in ML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          instruction  \\\n",
       "0   What is the fundamental goal of machine learning?   \n",
       "6   What is the difference between classification ...   \n",
       "11  What is feature engineering, and why is it imp...   \n",
       "15      What is a hyperparameter in machine learning?   \n",
       "90                          What is machine learning?   \n",
       "92  Can you explain the difference between a featu...   \n",
       "93  What is the training and testing process in ma...   \n",
       "99  What is the difference between regression and ...   \n",
       "\n",
       "                                               output  \\\n",
       "0   The fundamental goal of machine learning is to...   \n",
       "6   Classification involves predicting categorical...   \n",
       "11  Feature engineering involves creating new feat...   \n",
       "15  A hyperparameter is a configuration setting ex...   \n",
       "90  Machine learning is a subset of artificial int...   \n",
       "92  In machine learning, a feature is an input var...   \n",
       "93  The training process involves using a labeled ...   \n",
       "99  Regression involves predicting a continuous ou...   \n",
       "\n",
       "                                  topic  difficulty  \n",
       "0              Goal of Machine Learning           1  \n",
       "6         Classification vs. Regression           1  \n",
       "11                  Feature Engineering           1  \n",
       "15                      Hyperparameters           1  \n",
       "90     Introduction to Machine Learning           1  \n",
       "92              Feature and Label in ML           1  \n",
       "93           Training and Testing in ML           1  \n",
       "99  Regression vs. Classification in ML           1  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append topic and difficulty to the dataframe\n",
    "df['topic'] = topics\n",
    "df['difficulty'] = difficulty\n",
    "df.head(3)\n",
    "\n",
    "# show difficulty where it is 3 \n",
    "df[df['difficulty'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    89\n",
       "1     8\n",
       "3     3\n",
       "Name: difficulty, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.difficulty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need more Difficult Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: What is the intuition behind the Expectation-Maximization (EM) algorithm in Gaussian Mixture Models (GMM)? \\n',\n",
       " 'Answer: The EM algorithm iteratively estimates the parameters of a Gaussian Mixture Model by alternating between the expectation step, where it computes the probability of each data point belonging to each cluster, and the maximization step, where it updates the parameters based on these probabilities. Topic: EM Algorithm in GMM; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Could you elucidate the role of activation functions in deep neural networks (DNNs)?\\n',\n",
       " 'Answer: Activation functions introduce non-linearity to DNNs, enabling them to learn complex patterns. Functions like ReLU, Sigmoid, and Tanh control the output of neurons, facilitating gradient-based optimization and allowing neural networks to approximate arbitrary functions. Topic: Activation Functions in DNNs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How do decision trees handle feature selection and split point determination?\\n',\n",
       " 'Answer: Decision trees select features and split points based on criteria like information gain or Gini impurity. They recursively partition data by choosing the feature and split point that maximize the homogeneity of the resulting subsets, ultimately creating a tree structure for decision-making. Topic: Feature Selection in Decision Trees; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: What is the role of dropout regularization in training convolutional neural networks (CNNs)?\\n',\n",
       " 'Answer: Dropout regularization randomly deactivates neurons during training, forcing the network to learn redundant representations. This prevents overfitting by promoting the robustness and generalization of CNNs, leading to better performance on unseen data. Topic: Dropout Regularization in CNNs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How does the backpropagation algorithm enable neural networks to learn from data?\\n',\n",
       " 'Answer: Backpropagation computes gradients of the loss function with respect to the network parameters layer by layer, using the chain rule. These gradients guide the optimization process, allowing neural networks to update their weights and biases in a direction that minimizes the error, thus learning from the data. Topic: Backpropagation in Neural Networks; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Explain the role of activation functions in neural networks. \\n',\n",
       " 'Answer: Activation functions introduce non-linearities to neural network models, enabling them to learn complex patterns and relationships within data. They determine the output of each neuron based on the weighted sum of its inputs. Topic: Activation Functions in Neural Networks; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: What is the significance of the bias term in linear regression models?\\n',\n",
       " 'Answer: The bias term in linear regression models allows for the model to capture the offset or intercept, ensuring flexibility in fitting the data by accounting for deviations from the origin. It contributes to the overall predictive power of the model. Topic: Bias Term in Linear Regression; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Can you elucidate the importance of regularization techniques in machine learning?\\n',\n",
       " \"Answer: Regularization techniques help prevent overfitting by penalizing complex models, thus promoting simpler models that generalize well to unseen data. They strike a balance between model complexity and performance, enhancing the model's robustness. Topic: Regularization Techniques in Machine Learning; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: How does the expectation-maximization (EM) algorithm work in Gaussian mixture models (GMM)?\\n',\n",
       " 'Answer: The EM algorithm iteratively estimates the parameters of Gaussian mixture models by maximizing the likelihood function. In the E-step, it computes the expected value of the latent variables, while in the M-step, it updates the parameters based on these expectations. Topic: EM Algorithm in Gaussian Mixture Models; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Discuss the concept of dropout regularization in deep learning architectures.\\n',\n",
       " \"Answer: Dropout regularization randomly deactivates a fraction of neurons during training, preventing co-adaptation and enhancing model generalization by encouraging robust feature representations. It helps prevent overfitting and improves the model's ability to generalize to unseen data. Topic: Dropout Regularization in Deep Learning; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: How does the backpropagation algorithm optimize neural network parameters during training?\\n',\n",
       " 'Answer: The backpropagation algorithm computes gradients of the loss function with respect to each parameter in the neural network by recursively applying the chain rule. It then updates the parameters using gradient descent or its variants to minimize the loss, iteratively adjusting the model to better fit the training data. Topic: Backpropagation in Neural Networks; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: What is the role of the Kullback-Leibler divergence in variational autoencoders (VAEs)?\\n',\n",
       " 'Answer: The Kullback-Leibler divergence measures the difference between the distribution of latent variables learned by the variational autoencoder and a predefined prior distribution. Minimizing this divergence during training encourages the learned latent space to closely match the prior distribution, facilitating efficient and structured representation learning. Topic: Kullback-Leibler Divergence in VAEs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Explain the concept of transfer learning in convolutional neural networks (CNNs).\\n',\n",
       " 'Answer: Transfer learning involves leveraging pre-trained CNN models by fine-tuning them on a new task or dataset. By transferring knowledge learned from a source domain, typically a large dataset, to a target domain with limited data, transfer learning accelerates training and improves generalization performance, especially in scenarios with insufficient labeled data. Topic: Transfer Learning in CNNs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How do attention mechanisms enhance the performance of sequence-to-sequence models in natural language processing (NLP)?\\n',\n",
       " \"Answer: Attention mechanisms enable sequence-to-sequence models to selectively focus on relevant parts of the input sequence when generating an output sequence. By dynamically weighting input representations based on their importance, attention mechanisms improve the model's ability to capture long-range dependencies and generate more accurate and contextually relevant translations or summaries. Topic: Attention Mechanisms in NLP; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: Can you describe the role of ensemble learning methods in improving model performance?\\n',\n",
       " 'Answer: Ensemble learning methods combine predictions from multiple individual models to produce a final prediction that often outperforms any single constituent model. By leveraging diverse base learners and aggregating their outputs through techniques like bagging, boosting, or stacking, ensemble methods mitigate overfitting, reduce variance, and enhance the overall predictive accuracy and robustness of the model. Topic: Ensemble Learning for Model Improvement; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How does the Expectation-Maximization (EM) algorithm optimize parameters in the context of Gaussian Mixture Models (GMMs)?\\n',\n",
       " 'Answer: The EM algorithm iteratively estimates the parameters of Gaussian Mixture Models by alternating between the E-step and the M-step. In the E-step, it computes the posterior probabilities of latent variables given the current parameter estimates. In the M-step, it updates the parameters to maximize the expected log-likelihood computed in the E-step. This process continues until convergence, refining the parameter estimates and improving the model fit. Topic: EM Algorithm in GMMs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: What role do hyperparameters play in machine learning algorithms, and how are they optimized?\\n',\n",
       " 'Answer: Hyperparameters are settings that govern the behavior and performance of machine learning algorithms. They are not learned from the data but are set prior to training. Techniques such as grid search, random search, and Bayesian optimization are commonly used to search for optimal hyperparameters by systematically evaluating different combinations and selecting those that yield the best performance on a validation set. Topic: Hyperparameter Optimization in ML; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Explain the process of word embedding in natural language processing (NLP) tasks.\\n',\n",
       " 'Answer: Word embedding is a technique used to represent words as dense vectors in a continuous vector space. It captures semantic relationships between words by mapping them to vectors such that similar words are closer together in the embedding space. Popular methods like Word2Vec, GloVe, and fastText learn these embeddings from large text corpora by predicting context words given target words or using co-occurrence statistics. Topic: Word Embedding in NLP; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How do autoencoders learn efficient representations of input data?\\n',\n",
       " 'Answer: Autoencoders are neural network architectures trained to reconstruct their input data at the output layer. By learning to encode and decode data through a bottleneck layer with a lower dimensionality than the input, autoencoders capture salient features and patterns in the data while discarding noise and irrelevant information. The network learns to minimize the reconstruction error, resulting in compact and informative representations of the input data. Topic: Autoencoder Learning Process; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Discuss the importance of cross-validation techniques in machine learning model evaluation.\\n',\n",
       " \"Answer: Cross-validation techniques partition the dataset into multiple subsets, iteratively using one subset for validation while training the model on the remaining data. This process helps assess the model's generalization performance across different subsets and reduces the risk of overfitting to a specific training-validation split. Common methods such as k-fold cross-validation and leave-one-out cross-validation provide robust estimates of the model's performance and help in hyperparameter tuning and model selection. Topic: Cross-Validation in Model Evaluation; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: How does the Monte Carlo method facilitate Bayesian inference in machine learning?\\n',\n",
       " 'Answer: The Monte Carlo method approximates complex integrals and computes posterior distributions by drawing random samples from probability distributions. In Bayesian inference, it enables the estimation of posterior distributions by generating samples from the prior and likelihood distributions, allowing practitioners to make probabilistic inferences and quantify uncertainty in model parameters. Topic: Monte Carlo Method in Bayesian Inference; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: What is the significance of the F1 score in evaluating the performance of classification models?\\n',\n",
       " \"Answer: The F1 score, which is the harmonic mean of precision and recall, provides a balanced measure of a classification model's accuracy and completeness. It considers both false positives and false negatives, making it suitable for imbalanced datasets and scenarios where class distribution varies. By incorporating precision and recall, the F1 score offers a single metric to assess the model's overall effectiveness in correctly classifying instances. Topic: F1 Score in Classification Evaluation; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: Explain the concept of sparsity regularization in machine learning algorithms.\\n',\n",
       " \"Answer: Sparsity regularization encourages models to prefer sparse solutions by penalizing large coefficients or activations, promoting simplicity and interpretability. Techniques like L1 regularization introduce sparsity by adding the absolute values of coefficients to the loss function, driving irrelevant features to zero and selecting the most relevant ones. Sparsity regularization helps mitigate overfitting and enhances the model's ability to generalize to new data while reducing computational complexity. Topic: Sparsity Regularization in ML; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: How do deep reinforcement learning algorithms, such as Deep Q-Networks (DQN), balance exploration and exploitation?\\n',\n",
       " 'Answer: Deep reinforcement learning algorithms balance exploration (discovering new strategies) and exploitation (leveraging known strategies) through mechanisms like ε-greedy exploration, where the agent selects a random action with probability ε and the action with the highest estimated value otherwise. By gradually decaying ε over time or using other exploration strategies, DQN agents learn to explore the environment effectively while leveraging learned policies to maximize cumulative rewards. Topic: Exploration-Exploitation Balance in DRL; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Can you elucidate the role of word attention mechanisms in neural machine translation (NMT) systems?\\n',\n",
       " 'Answer: Word attention mechanisms in NMT systems dynamically allocate attention weights to source words during translation, allowing the model to focus on relevant information and generate contextually appropriate target words. By attending to different parts of the source sentence at each decoding step, word attention mechanisms improve translation quality, handle long input sequences effectively, and facilitate the alignment of words across languages, enhancing the overall performance of NMT systems. Topic: Word Attention Mechanisms in NMT; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How does the AdaBoost algorithm combine weak learners to form a strong classifier?\\n',\n",
       " 'Answer: AdaBoost assigns weights to training instances and iteratively trains weak learners on subsets of data where the weights of misclassified instances are increased. In subsequent iterations, it gives higher weights to misclassified instances, focusing the subsequent weak learners on the most challenging examples. Finally, AdaBoost combines the weak learners into a strong classifier by weighting their predictions based on their individual performance. Topic: AdaBoost Algorithm; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Discuss the concept of kernel density estimation (KDE) and its application in non-parametric density estimation.\\n',\n",
       " 'Answer: Kernel density estimation (KDE) estimates the probability density function of a random variable by placing a kernel function at each data point and summing them to create a smooth estimate of the underlying distribution. It is particularly useful in non-parametric density estimation when the distribution of the data is unknown or cannot be easily modeled with traditional parametric methods. KDE allows for flexible and data-driven estimation of probability densities. Topic: Kernel Density Estimation; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How do generative adversarial networks (GANs) achieve realistic data generation through adversarial training?\\n',\n",
       " 'Answer: In generative adversarial networks (GANs), a generator network learns to generate samples from a target distribution, while a discriminator network learns to distinguish between real and generated samples. Through adversarial training, the generator improves by generating samples that increasingly fool the discriminator, while the discriminator improves by better distinguishing real from fake samples. This adversarial process drives the generator to produce more realistic samples, leading to high-quality data generation. Topic: Generative Adversarial Networks; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Explain the concept of attention-based mechanisms in transformer architectures for sequence-to-sequence tasks.\\n',\n",
       " 'Answer: Attention mechanisms in transformer architectures allow models to focus on different parts of the input sequence when producing each output token. By computing attention scores between every pair of input and output positions, transformers capture dependencies across the entire input and output sequences, enabling effective handling of long-range dependencies and facilitating parallelization during training and inference. Attention mechanisms enhance the performance of transformer models in various sequence-to-sequence tasks, including machine translation and text summarization. Topic: Attention Mechanisms in Transformers; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Can you elaborate on the role of dropout regularization in training deep neural networks?\\n',\n",
       " 'Answer: Dropout regularization randomly deactivates a fraction of neurons during training, preventing co-adaptation and overfitting by forcing the network to learn redundant representations. By implicitly training an ensemble of subnetworks, dropout introduces robustness and generalization ability to deep neural networks, improving their performance on unseen data. Dropout regularization is a widely-used technique in deep learning, particularly for reducing overfitting and enhancing the generalization capability of models. Topic: Dropout Regularization in Deep Learning; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How does the Expectation-Maximization (EM) algorithm converge to optimal parameters in Gaussian Mixture Models (GMMs)?\\n',\n",
       " 'Answer: The EM algorithm iteratively maximizes the likelihood function to estimate the parameters of Gaussian Mixture Models (GMMs). In the E-step, it computes the expected values of latent variables given the current parameter estimates. In the M-step, it updates the parameters based on these expectations. This iterative process continues until convergence, where the likelihood converges to a local maximum, yielding optimal parameter estimates for the GMM. Topic: EM Algorithm Convergence in GMMs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Discuss the role of the softmax activation function in multi-class classification tasks.\\n',\n",
       " 'Answer: The softmax activation function transforms raw output scores into a probability distribution over multiple classes, ensuring that the predicted probabilities sum up to one. It is commonly used in multi-class classification tasks to convert raw model outputs into class probabilities, allowing for intuitive interpretation and enabling the selection of the most likely class. The softmax function helps in decision-making by providing a clear representation of class likelihoods. Topic: Softmax Activation in Classification; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Can you explain the concept of early stopping in training neural networks?\\n',\n",
       " \"Answer: Early stopping is a regularization technique used during neural network training to prevent overfitting. It involves monitoring the model's performance on a validation set and halting training when the performance starts to degrade, typically by measuring validation loss. Early stopping allows the model to avoid overfitting by stopping training before it learns noise in the training data, thereby improving generalization performance on unseen data. Topic: Early Stopping in Neural Networks; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: What is the role of the learning rate in gradient descent optimization algorithms?\\n',\n",
       " 'Answer: The learning rate determines the step size taken during gradient descent optimization, influencing the rate at which the model parameters are updated. A high learning rate may lead to overshooting the minimum, while a low learning rate may result in slow convergence. Finding an appropriate learning rate is crucial for effective optimization, as it balances convergence speed and stability in gradient descent algorithms. Topic: Learning Rate in Gradient Descent; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Explain the concept of batch normalization and its significance in deep neural networks.\\n',\n",
       " 'Answer: Batch normalization is a technique used to standardize the inputs to each layer of a neural network by normalizing the activations based on the mean and variance of each mini-batch during training. It helps stabilize the training process by reducing internal covariate shift, accelerating convergence, and improving the overall performance of deep neural networks. Batch normalization enables deeper networks to be trained more effectively by mitigating the vanishing/exploding gradient problem. Topic: Batch Normalization in DNNs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Question: How does the Lasso regression technique promote sparsity in feature selection?\\n',\n",
       " \"Answer: Lasso regression imposes an L1 penalty on the absolute values of the regression coefficients during training. This penalty encourages the coefficients of less important features to shrink towards zero, effectively eliminating some features from the model. By promoting sparsity in the coefficient vector, Lasso regression facilitates automatic feature selection and improves the model's interpretability. Topic: Lasso Regression for Sparsity; Difficulty: 3\\n\",\n",
       " '\\n',\n",
       " 'Question: Discuss the role of the Adam optimization algorithm in training deep neural networks.\\n',\n",
       " 'Answer: The Adam optimization algorithm combines the advantages of adaptive learning rates and momentum-based optimization. It maintains per-parameter adaptive learning rates that adapt based on the estimates of the first and second moments of gradients. By incorporating both momentum and adaptive learning rates, Adam effectively navigates complex optimization landscapes, accelerates convergence, and improves the training efficiency of deep neural networks. Topic: Adam Optimization in DNNs; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Can you explain the concept of word embeddings and their applications in natural language processing (NLP)?\\n',\n",
       " 'Answer: Word embeddings are dense vector representations of words that capture semantic relationships and contextual information. They encode semantic similarity and syntactic patterns, enabling NLP models to better understand and process textual data. Word embeddings find applications in various NLP tasks such as sentiment analysis, machine translation, named entity recognition, and document classification, where they help improve model performance by providing meaningful representations of words. Topic: Word Embeddings in NLP; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: How do decision trees handle categorical variables during the splitting process?\\n',\n",
       " 'Answer: Decision trees handle categorical variables by performing multi-way splits, where each branch corresponds to one of the categories in the variable. During the splitting process, the tree algorithm evaluates possible splits based on each category, choosing the split that maximizes the information gain or minimizes impurity. By considering all categories, decision trees effectively partition the data based on categorical variables and create branches accordingly, leading to accurate and interpretable models. Topic: Decision Trees and Categorical Variables; Difficulty: 3\\n',\n",
       " '\\n',\n",
       " 'Question: Explain the concept of a recurrent neural network (RNN) and its suitability for sequential data processing tasks.\\n',\n",
       " 'Answer: Recurrent neural networks (RNNs) are neural network architectures designed to process sequential data by maintaining internal state or memory. They utilize feedback loops that allow information to persist across time steps, enabling them to capture temporal dependencies and patterns in sequential data. RNNs find applications in tasks such as language modeling, time series prediction, speech recognition, and natural language processing, where they excel at processing sequences of variable lengths and capturing long-term dependencies. Topic: Recurrent Neural Networks for Sequential Data; Difficulty: 3\\n']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from difficultqns.txt\n",
    "with open('difficultqns.txt') as f:\n",
    "    difficultqns = f.readlines()\n",
    "\n",
    "difficultqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The EM algorithm iteratively estimates the parameters of a Gaussian Mixture Model by alternating between the expectation step, where it computes the probability of each data point belonging to each cluster, and the maximization step, where it updates the parameters based on these probabilities. Topic: EM Algorithm in GMM; Difficulty: 3'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "instructions = []\n",
    "output = []\n",
    "topics = []\n",
    "for line in difficultqns:\n",
    "    if 'Question:' in line:\n",
    "        instructions.append(line.split('Question:')[1].strip())\n",
    "    else:\n",
    "        instructions.append('')\n",
    "instructions = list(filter(None, instructions))\n",
    "\n",
    "for line in difficultqns:\n",
    "    if 'Answer:' in line:\n",
    "        output.append(line.split('Answer:')[1].strip())\n",
    "    else:\n",
    "        output.append('')\n",
    "output = list(filter(None, output))\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in output:\n",
    "    if 'Topic:' in line:\n",
    "        topics.append(line.split('Topic:')[1].strip().split(';')[0].strip())\n",
    "    else:\n",
    "        topics.append('')\n",
    "\n",
    "topics = list(filter(None, topics))\n",
    "topics\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a list of 3 based on the length of topics\n",
    "difficulty = [3] * len(topics)\n",
    "difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame({'instruction': instructions, 'output': output, 'topic': topics, 'difficulty': difficulty})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more easy questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What role does regularization play in machine learning models? Answer: Regularization helps prevent overfitting in machine learning models by adding a penalty term to the loss function, discouraging overly complex models. It balances between fitting the training data well and generalizing to unseen data. Topic: Regularization in ML; Difficulty:\\n',\n",
       " '\\n',\n",
       " \"How do decision trees work in machine learning? Answer: Decision trees partition the feature space into regions and make predictions by following a tree-like structure based on feature values. They recursively split the data into subsets to maximize information gain or minimize impurity, creating a predictive model that's easy to interpret. Topic: Decision Trees in ML; Difficulty:\\n\",\n",
       " '\\n',\n",
       " 'What is the purpose of cross-validation in machine learning? Answer: Cross-validation is a technique used to assess the performance of machine learning models by partitioning the dataset into multiple subsets. It helps to estimate how the model will perform on unseen data and provides insights into its generalization ability. Topic: Cross-validation in ML; Difficulty:\\n',\n",
       " '\\n',\n",
       " 'How do neural networks learn in machine learning? Answer: Neural networks learn by adjusting the weights and biases of interconnected nodes or neurons through iterative optimization algorithms like gradient descent. They minimize the difference between predicted and actual outcomes by updating parameters based on the error calculated during forward and backward propagation. Topic: Neural Network Learning in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How does unsupervised learning differ from supervised learning in machine learning? Answer: Unsupervised learning aims to find patterns or structures in data without explicit labels, while supervised learning involves learning from labeled data to make predictions or classifications. Unsupervised learning methods include clustering and dimensionality reduction, while supervised learning includes regression and classification tasks. Topic: Unsupervised vs. Supervised Learning; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"What is the role of activation functions in neural networks? Answer: Activation functions introduce non-linearity into neural networks, enabling them to learn complex relationships in data. Functions like ReLU, Sigmoid, and Tanh determine the output of each neuron, facilitating the network's ability to approximate arbitrary functions and capture intricate patterns in data. Topic: Activation Functions in Neural Networks; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'How do you handle imbalanced datasets in machine learning? Answer: Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using algorithmic methods like ensemble learning with techniques like SMOTE, and incorporating appropriate evaluation metrics like precision, recall, and F1-score to assess model performance accurately. Topic: Handling Imbalanced Datasets in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What is transfer learning in machine learning? Answer: Transfer learning involves leveraging knowledge gained from one task or domain to improve learning or performance in a different but related task or domain. It allows models trained on large datasets or tasks to be repurposed for new tasks with limited amounts of labeled data, speeding up training and improving generalization. Topic: Transfer Learning in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"Can you explain the bias-variance tradeoff in machine learning models? Answer: The bias-variance tradeoff refers to the balance between a model's ability to capture the underlying patterns in data (bias) and its sensitivity to variations in the training data (variance). Increasing model complexity reduces bias but increases variance, while decreasing complexity reduces variance but may increase bias. Achieving an optimal tradeoff is crucial for building models that generalize well to unseen data. Topic: Bias-Variance Tradeoff in ML; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " \"What is the difference between k-means and hierarchical clustering in unsupervised learning? Answer: K-means clustering aims to partition data into a pre-defined number of clusters by minimizing the sum of squared distances within each cluster, while hierarchical clustering builds a tree-like hierarchy of clusters by iteratively merging or splitting clusters based on their proximity. K-means requires the number of clusters as input, while hierarchical clustering doesn't. Topic: K-means vs. Hierarchical Clustering; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'How does the Support Vector Machine (SVM) algorithm work in machine learning? Answer: SVM constructs a hyperplane in a high-dimensional space to separate classes by maximizing the margin between the nearest data points (support vectors). It aims to find the optimal hyperplane that best separates the data while minimizing classification errors. SVM can handle linear and non-linear classification tasks using different kernel functions like linear, polynomial, or radial basis function (RBF). Topic: Support Vector Machine (SVM) in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What are the key components of a Convolutional Neural Network (CNN)? Answer: CNNs consist of convolutional layers, which apply filters to extract spatial patterns from input data, pooling layers, which reduce spatial dimensions while retaining important information, and fully connected layers, which perform classification based on the extracted features. CNNs also often incorporate activation functions like ReLU and regularization techniques like dropout to improve performance and prevent overfitting. Topic: Components of CNN; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How do you address the curse of dimensionality in machine learning? Answer: The curse of dimensionality refers to the increased complexity and sparsity of data as the number of dimensions grows. Techniques for addressing it include feature selection and dimensionality reduction methods like principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE). These methods help reduce the number of features or dimensions while preserving the most relevant information, improving model performance and efficiency. Topic: Curse of Dimensionality in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What is ensemble learning, and how does it improve machine learning models? Answer: Ensemble learning combines multiple base models to make predictions more robust and accurate than individual models alone. Techniques like bagging (bootstrap aggregating), boosting, and stacking leverage the diversity of base models to reduce variance, minimize bias, and enhance generalization performance on unseen data. Popular ensemble methods include Random Forest, AdaBoost, and Gradient Boosting Machines (GBM). Topic: Ensemble Learning \\n',\n",
       " '\\n',\n",
       " 'What are the main steps involved in the preprocessing of textual data for natural language processing (NLP) tasks? Answer: Textual data preprocessing typically includes steps such as tokenization, lowercasing, removing punctuation and stop words, stemming or lemmatization, and handling special characters or entities. These steps help standardize text data and prepare it for further analysis or modeling in NLP tasks like sentiment analysis or text classification. Topic: Textual Data Preprocessing for NLP; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"How does the concept of overfitting manifest in deep learning models, particularly neural networks? Answer: Overfitting in deep learning occurs when a model learns to memorize the training data's noise and outliers rather than capturing the underlying patterns. This leads to poor generalization performance, where the model performs well on the training data but fails to generalize to unseen data. Techniques to mitigate overfitting include dropout, early stopping, regularization, and data augmentation. Topic: Overfitting in Deep Learning Models; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'Can you explain the concept of word embeddings in natural language processing (NLP)? Answer: Word embeddings represent words as dense vectors in a continuous vector space, where similar words are mapped to nearby points. Techniques like Word2Vec, GloVe, and FastText learn word embeddings by considering the context in which words appear in large text corpora, capturing semantic relationships and syntactic patterns. Word embeddings are used to enhance NLP tasks like semantic similarity, text classification, and named entity recognition. Topic: Word Embeddings in NLP; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How does the Long Short-Term Memory (LSTM) network address the vanishing gradient problem in recurrent neural networks (RNNs)? Answer: LSTM networks use a gated architecture with three gates (input, forget, and output gates) and a memory cell to selectively retain or discard information over time. This allows LSTMs to learn and remember long-term dependencies in sequential data while mitigating the vanishing gradient problem by regulating the flow of gradients during backpropagation. LSTMs are commonly used in tasks like sequence modeling, language translation, and sentiment analysis. Topic: LSTM Networks and Vanishing Gradient Problem; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"What role does the loss function play in training machine learning models, and how do you choose an appropriate loss function for a specific task? Answer: The loss function quantifies the discrepancy between the model's predictions and the actual target values during training. It serves as a measure of the model's performance and guides the optimization process by updating model parameters to minimize the loss. Choosing an appropriate loss function depends on the task at hand, such as regression, classification, or sequence generation, and the desired properties of the model's output. Common loss functions include mean squared error, cross-entropy loss, and hinge loss. Topic: Loss Functions in ML; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'What are the advantages and disadvantages of using ensemble methods like Random Forest compared to single decision tree models? Answer: Ensemble methods like Random Forest offer improved robustness against overfitting, better generalization performance, and ability to handle high-dimensional data compared to single decision tree models. However, Random Forest may be computationally more expensive, less interpretable, and prone to bias if the base models are biased. Understanding the trade-offs helps in selecting the appropriate model for a given task. Topic: Random Forest vs. Single Decision Tree Models; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How does batch normalization contribute to the training stability and performance of deep neural networks? Answer: Batch normalization normalizes the activations of each layer across mini-batches during training, reducing internal covariate shift and ensuring stable gradient flow. It accelerates convergence, allows the use of higher learning rates, and acts as a form of regularization, improving the generalization performance of deep neural networks. By maintaining activation distributions, batch normalization facilitates faster training and better performance on various tasks. Topic: Batch Normalization in Deep Learning; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What is the purpose of hyperparameter tuning in machine learning, and what techniques can be used to optimize hyperparameters effectively? Answer: Hyperparameter tuning involves selecting the optimal values for hyperparameters that control the learning process of machine learning algorithms. Techniques like grid search, random search, Bayesian optimization, and evolutionary algorithms can be used to search the hyperparameter space efficiently and find configurations that yield the best model performance on validation data. Hyperparameter tuning helps improve model generalization and performance across different datasets. Topic: Hyperparameter Tuning in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"How do attention mechanisms enhance the performance of sequence-to-sequence models in natural language processing tasks such as machine translation and text summarization? Answer: Attention mechanisms allow sequence-to-sequence models to focus on relevant parts of the input sequence when generating the output sequence. By dynamically weighting input representations based on their importance, attention mechanisms capture long-range dependencies and improve the model's ability to generate accurate and coherent translations or summaries. Attention mechanisms have revolutionized sequence-to-sequence modeling and significantly improved the performance of NLP tasks. Topic: Attention Mechanisms in Sequence-to-Sequence Models; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " \"What are some common evaluation metrics used for assessing the performance of classification models, and how do they differ in terms of interpretability and sensitivity to class imbalance? Answer: Common evaluation metrics for classification models include accuracy, precision, recall, F1-score, and ROC-AUC. Accuracy measures the overall correctness of predictions, while precision and recall focus on the positive class's identification. F1-score balances precision and recall, making it suitable for imbalanced datasets. ROC-AUC measures the model's ability to distinguish between classes across different thresholds. Understanding the strengths and limitations of each metric helps in evaluating classification models effectively. Topic: Evaluation Metrics for Classification Models; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'How does the concept of transfer learning apply to computer vision tasks, and what are some popular pre-trained convolutional neural network architectures used for transfer learning? Answer: Transfer learning in computer vision involves leveraging pre-trained CNN models trained on large datasets like ImageNet to extract generic features, which are then fine-tuned on a target dataset for specific tasks. Popular pre-trained CNN architectures for transfer learning include VGG, ResNet, Inception, and MobileNet, which offer varying trade-offs between model size, computational efficiency, and performance. Transfer learning accelerates model training and improves performance, especially with limited labeled data. Topic: Transfer Learning in Computer Vision; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"Can you explain the concept of attention mechanisms in deep learning, and how are they used in sequence-to-sequence models for tasks like machine translation and text summarization? Answer: Attention mechanisms allow sequence-to-sequence models to focus on relevant parts of the input sequence when generating each output token. By dynamically weighting input representations based on their importance, attention mechanisms capture long-range dependencies and improve the model's ability to generate accurate translations or summaries. In tasks like machine translation and text summarization, attention mechanisms help the model align source and target sequences effectively, enhancing translation quality and coherence. Topic: Attention Mechanisms in Sequence-to-Sequence Models; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " \"What is the purpose of data augmentation in training deep learning models, particularly for image classification tasks, and what are some common augmentation techniques used? Answer: Data augmentation involves generating synthetic training samples by applying transformations like rotation, flipping, scaling, cropping, and color jittering to original images. The purpose of data augmentation is to increase the diversity of the training data and improve the model's generalization performance by exposing it to various data variations. Common augmentation techniques used in image classification tasks help reduce overfitting, enhance model robustness, and boost performance, especially when training datasets are limited. Topic: Data Augmentation in Deep Learning; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'How do recurrent neural networks (RNNs) differ from convolutional neural networks (CNNs), and what are some applications where RNNs excel compared to CNNs? Answer: Recurrent neural networks (RNNs) are designed to process sequential data by maintaining internal state and capturing temporal dependencies, making them suitable for tasks like time series forecasting, natural language processing, and speech recognition. In contrast, convolutional neural networks (CNNs) excel at capturing spatial patterns in data like images and are widely used for tasks such as image classification, object detection, and semantic segmentation. While CNNs focus on spatial hierarchies, RNNs model temporal dependencies, making them complementary for tasks requiring sequential data processing. Topic: RNNs vs. CNNs and Applications; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What are the key challenges associated with training deep learning models on resource-constrained devices like mobile phones or IoT devices, and what strategies can be employed to address these challenges? Answer: Training deep learning models on resource-constrained devices faces challenges such as limited computational power, memory constraints, and energy consumption considerations. To address these challenges, techniques like model compression, quantization, pruning, and knowledge distillation are employed to reduce model size and complexity while maintaining performance. Additionally, deploying lightweight architectures and leveraging hardware accelerators like GPUs or TPUs help optimize model inference on resource-constrained devices without compromising efficiency or accuracy. Topic: Training Deep Learning Models on Resource-Constrained Devices; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " \"How does the concept of reinforcement learning differ from supervised and unsupervised learning, and what are some real-world applications where reinforcement learning excels? Answer: Reinforcement learning involves an agent interacting with an environment, learning to make sequential decisions to maximize cumulative rewards. Unlike supervised learning, reinforcement learning doesn't require labeled data, and unlike unsupervised learning, it focuses on learning to take actions to achieve specific goals. Reinforcement learning excels in applications like game playing (e.g., AlphaGo), robotics control, autonomous driving, and recommendation systems where decision-making in dynamic environments is crucial. Topic: Reinforcement Learning vs. Supervised and Unsupervised Learning; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " \"Can you explain the concept of GANs (Generative Adversarial Networks) in deep learning, and how do they work to generate realistic data samples? Answer: GANs consist of two neural networks, a generator and a discriminator, engaged in a minimax game. The generator learns to generate synthetic data samples that mimic the distribution of real data, while the discriminator learns to distinguish between real and fake samples. Through adversarial training, GANs iteratively improve both the generator's ability to produce realistic samples and the discriminator's ability to differentiate real from fake samples, resulting in high-quality synthesized data. Topic: Generative Adversarial Networks (GANs) in Deep Learning; Difficulty: 1\\n\",\n",
       " '\\n',\n",
       " 'What are the advantages and disadvantages of using convolutional neural networks (CNNs) compared to traditional computer vision techniques for image processing tasks? Answer: Convolutional neural networks (CNNs) excel at automatically learning hierarchical representations directly from raw pixel data, eliminating the need for handcrafted features used in traditional computer vision techniques. CNNs can capture complex patterns and relationships in images, enabling superior performance in tasks like image classification, object detection, and semantic segmentation. However, CNNs require large amounts of labeled data and computational resources for training, and they lack interpretability compared to traditional techniques. Topic: CNNs vs. Traditional Computer Vision Techniques; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How does the concept of transfer learning apply to natural language processing (NLP) tasks, and what are some popular pre-trained language models used for transfer learning in NLP? Answer: Transfer learning in NLP involves fine-tuning pre-trained language models on specific downstream tasks, leveraging the knowledge learned from large-scale language modeling tasks like masked language modeling or next sentence prediction. Popular pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and RoBERTa serve as strong starting points for various NLP tasks such as text classification, sentiment analysis, and named entity recognition. Topic: Transfer Learning in Natural Language Processing (NLP); Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What are some techniques for handling imbalanced datasets in machine learning, and how do they help improve model performance on minority classes? Answer: Techniques for handling imbalanced datasets include resampling methods like oversampling the minority class (e.g., SMOTE) and undersampling the majority class, using class weights to penalize misclassifications of minority samples, and employing ensemble methods like bagging and boosting. These techniques help balance the class distribution in the training data, prevent the model from being biased towards the majority class, and improve its ability to correctly classify minority class instances. Topic: Handling Imbalanced Datasets in Machine Learning; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What are the key components of a graph neural network (GNN), and how do they enable effective representation learning on graph-structured data? Answer: Graph neural networks (GNNs) consist of graph convolutional layers that aggregate and propagate information across neighboring nodes, node-level and graph-level aggregation functions, and optional attention mechanisms to focus on relevant nodes or edges. By iteratively updating node representations based on their local neighborhood structure and global graph topology, GNNs capture rich relational information and facilitate effective representation learning on graph-structured data for tasks like node classification, link prediction, and graph classification. Topic: Graph Neural Networks (GNNs) and Representation Learning; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'Can you explain the concept of zero-shot learning in machine learning, and what are some techniques used to address the challenges associated with learning from unseen classes or domains? Answer: Zero-shot learning aims to recognize or classify instances belonging to classes not present during training, relying on semantic attributes, textual descriptions, or external knowledge sources to generalize across unseen classes or domains. Techniques for zero-shot learning include attribute-based methods, generative models, and meta-learning approaches that leverage auxiliary information to infer relationships between seen and unseen classes, enabling effective knowledge transfer and adaptation to novel tasks or domains. Topic: Zero-Shot Learning in Machine Learning; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How do autoencoders contribute to unsupervised learning tasks in deep learning, and what are some applications where autoencoders are commonly used? Answer: Autoencoders are neural network architectures trained to reconstruct input data from compressed representations, effectively learning compact and informative representations of input data without explicit supervision. They can be used for dimensionality reduction, data denoising, anomaly detection, and feature learning tasks across various domains such as image processing, natural language processing, and signal processing. By capturing intrinsic structures and patterns in data, autoencoders enable unsupervised learning and facilitate downstream tasks like clustering, classification, and visualization. Topic: Autoencoders in Unsupervised Learning; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'What are the differences between generative and discriminative models in machine learning, and how do their learning approaches and applications vary? Answer: Generative models learn the joint probability distribution of input features and labels, enabling them to generate new data samples and perform tasks like data generation, denoising, and imputation. Discriminative models, on the other hand, directly model the conditional probability of labels given input features, focusing on classification and prediction tasks without explicit generation capabilities. While generative models offer richer representation learning and synthesis capabilities, discriminative models often exhibit superior performance in classification and regression tasks, depending on the available data and task requirements. Topic: Generative vs. Discriminative Models in ML; Difficulty: 1\\n',\n",
       " '\\n',\n",
       " 'How does the concept of self-supervised learning contribute to training deep learning models, and what are some popular techniques used for self-supervised learning across different domains? Answer: Self-supervised learning involves training models to predict missing or corrupted parts of input data without explicit supervision, leveraging inherent structures or relationships within the data itself. Popular self-supervised learning techniques include pretext tasks like image inpainting, context prediction, sequence prediction, and contrastive learning, where models learn meaningful representations by maximizing agreement between different views or augmentations of the same data samples. Self-supervised learning enables effective pretraining of deep learning models and improves generalization performance on downstream tasks by capturing rich and semantically meaningful representations from unlabeled data. Topic: Self-Supervised Learning in Deep Learning; Difficulty: 1']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from easyqns.txt\n",
    "with open('easyqns.txt') as f:\n",
    "    easyqns = f.readlines()\n",
    "\n",
    "easyqns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "output = []\n",
    "topics = []\n",
    "\n",
    "for line in easyqns:\n",
    "    if 'Answer:' in line:\n",
    "        instructions.append(line.split('Answer:')[0].strip())\n",
    "    else:\n",
    "        instructions.append('')\n",
    "instructions = list(filter(None,instructions))\n",
    "instructions\n",
    "\n",
    "for line in easyqns:\n",
    "    if 'Answer:' in line:\n",
    "        output.append(line.split('Answer:')[1].strip().split('Topic:')[0].strip())\n",
    "    else:\n",
    "        output.append('')\n",
    "output = list(filter(None, output))\n",
    "\n",
    "for line in easyqns:\n",
    "    if 'Topic:' in line:\n",
    "        topics.append(line.split('Topic:')[1].strip().split(';')[0].strip())\n",
    "    else:\n",
    "        topics.append('')\n",
    "    \n",
    "topics = list(filter(None, topics))\n",
    "\n",
    "len(topics)\n",
    "\n",
    "difficulty = [1] * len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>topic</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the fundamental goal of machine learning?</td>\n",
       "      <td>The fundamental goal of machine learning is to...</td>\n",
       "      <td>Goal of Machine Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain the concept of overfitting in machine ...</td>\n",
       "      <td>Overfitting occurs when a machine learning mod...</td>\n",
       "      <td>Overfitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you distinguish between bias and variance ...</td>\n",
       "      <td>Bias refers to the error introduced by approxi...</td>\n",
       "      <td>Bias and variance</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of a loss function in machine...</td>\n",
       "      <td>A loss function measures the difference betwee...</td>\n",
       "      <td>Loss Function</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the concept of gradient descent.</td>\n",
       "      <td>Gradient descent is an optimization algorithm ...</td>\n",
       "      <td>Gradient Descent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What are the key components of a graph neural ...</td>\n",
       "      <td>Graph neural networks (GNNs) consist of graph ...</td>\n",
       "      <td>Graph Neural Networks (GNNs) and Representatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Can you explain the concept of zero-shot learn...</td>\n",
       "      <td>Zero-shot learning aims to recognize or classi...</td>\n",
       "      <td>Zero-Shot Learning in Machine Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>How do autoencoders contribute to unsupervised...</td>\n",
       "      <td>Autoencoders are neural network architectures ...</td>\n",
       "      <td>Autoencoders in Unsupervised Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What are the differences between generative an...</td>\n",
       "      <td>Generative models learn the joint probability ...</td>\n",
       "      <td>Generative vs. Discriminative Models in ML</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>How does the concept of self-supervised learni...</td>\n",
       "      <td>Self-supervised learning involves training mod...</td>\n",
       "      <td>Self-Supervised Learning in Deep Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          instruction  \\\n",
       "0   What is the fundamental goal of machine learning?   \n",
       "1   Explain the concept of overfitting in machine ...   \n",
       "2   Can you distinguish between bias and variance ...   \n",
       "3   What is the role of a loss function in machine...   \n",
       "4            Explain the concept of gradient descent.   \n",
       "..                                                ...   \n",
       "34  What are the key components of a graph neural ...   \n",
       "35  Can you explain the concept of zero-shot learn...   \n",
       "36  How do autoencoders contribute to unsupervised...   \n",
       "37  What are the differences between generative an...   \n",
       "38  How does the concept of self-supervised learni...   \n",
       "\n",
       "                                               output  \\\n",
       "0   The fundamental goal of machine learning is to...   \n",
       "1   Overfitting occurs when a machine learning mod...   \n",
       "2   Bias refers to the error introduced by approxi...   \n",
       "3   A loss function measures the difference betwee...   \n",
       "4   Gradient descent is an optimization algorithm ...   \n",
       "..                                                ...   \n",
       "34  Graph neural networks (GNNs) consist of graph ...   \n",
       "35  Zero-shot learning aims to recognize or classi...   \n",
       "36  Autoencoders are neural network architectures ...   \n",
       "37  Generative models learn the joint probability ...   \n",
       "38  Self-supervised learning involves training mod...   \n",
       "\n",
       "                                                topic  difficulty  \n",
       "0                            Goal of Machine Learning           1  \n",
       "1                                         Overfitting           2  \n",
       "2                                   Bias and variance           2  \n",
       "3                                       Loss Function           3  \n",
       "4                                    Gradient Descent           2  \n",
       "..                                                ...         ...  \n",
       "34  Graph Neural Networks (GNNs) and Representatio...           1  \n",
       "35             Zero-Shot Learning in Machine Learning           1  \n",
       "36              Autoencoders in Unsupervised Learning           1  \n",
       "37         Generative vs. Discriminative Models in ML           1  \n",
       "38          Self-Supervised Learning in Deep Learning           1  \n",
       "\n",
       "[179 rows x 4 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, pd.DataFrame({'instruction': instructions, 'output': output, 'topic': topics, 'difficulty': difficulty})])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>topic</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>How do autoencoders contribute to unsupervised...</td>\n",
       "      <td>Autoencoders are neural network architectures ...</td>\n",
       "      <td>Autoencoders in Unsupervised Learning</td>\n",
       "      <td>1</td>\n",
       "      <td>seed_task_176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>What are the differences between generative an...</td>\n",
       "      <td>Generative models learn the joint probability ...</td>\n",
       "      <td>Generative vs. Discriminative Models in ML</td>\n",
       "      <td>1</td>\n",
       "      <td>seed_task_177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>How does the concept of self-supervised learni...</td>\n",
       "      <td>Self-supervised learning involves training mod...</td>\n",
       "      <td>Self-Supervised Learning in Deep Learning</td>\n",
       "      <td>1</td>\n",
       "      <td>seed_task_178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  \\\n",
       "176  How do autoencoders contribute to unsupervised...   \n",
       "177  What are the differences between generative an...   \n",
       "178  How does the concept of self-supervised learni...   \n",
       "\n",
       "                                                output  \\\n",
       "176  Autoencoders are neural network architectures ...   \n",
       "177  Generative models learn the joint probability ...   \n",
       "178  Self-supervised learning involves training mod...   \n",
       "\n",
       "                                          topic  difficulty             id  \n",
       "176       Autoencoders in Unsupervised Learning           1  seed_task_176  \n",
       "177  Generative vs. Discriminative Models in ML           1  seed_task_177  \n",
       "178   Self-Supervised Learning in Deep Learning           1  seed_task_178  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column called 'id' and set it to the index of the dataframe\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['id'] = df.index\n",
    "df['id'] = \"seed_task_\" + df['id'].astype(str)\n",
    "df.tail(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to jsonl for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert DataFrame to JSON Lines (JSONL)\n",
    "with open('raw.jsonl', 'w') as f:\n",
    "    df.to_json(f, orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSONL file and transform each line\n",
    "with open('raw.jsonl', 'r') as f:\n",
    "    output_tasks = []\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        topic_difficulty = f\"{data['topic']} - Difficulty: {data['difficulty']}\"\n",
    "        difficulty_level = \"Beginner\" if data[\"difficulty\"] == 1 else \"Intermediate\" if data[\"difficulty\"] == 2 else \"Advanced\"\n",
    "        new_task = {\n",
    "            \"id\": data[\"id\"],\n",
    "            \"instruction\": data[\"instruction\"],\n",
    "            \"info\": [{\n",
    "                \"difficulty\": difficulty_level,\n",
    "                \"topic\": topic_difficulty\n",
    "            }],\n",
    "            \"output\": data[\"output\"]\n",
    "        }\n",
    "        output_tasks.append(new_task)\n",
    "\n",
    "# Write the transformed tasks to a new JSONL file\n",
    "with open('output.jsonl', 'w') as f:\n",
    "    for task in output_tasks:\n",
    "        json.dump(task, f)\n",
    "        f.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_envv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
